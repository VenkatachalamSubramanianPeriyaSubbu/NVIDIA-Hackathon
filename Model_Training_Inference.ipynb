{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c9fc31-b77e-4007-8901-d186024675cd",
   "metadata": {},
   "source": [
    "# Step 3: Training a LoRA Adapter\n",
    "\n",
    "This notebook performs the preparatory tasks needed for obtaining the base model that we will use for fine-tuning.\n",
    "\n",
    "This notebook showcases performing LoRA fine-tuning on the dataset that we curated in step 1.\n",
    "\n",
    "## Setup and Requirements\n",
    "Before proceeding, please make ensure you have completed the notebooks for steps 1 and 2. You will need to install one dependency to follow along. Execute the following cell before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db9f6f9-b0c3-4c15-92d4-5c7ff0e5a286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8e64b",
   "metadata": {},
   "source": [
    "Let's also specify the base model name that we will use for fine-tuning. This should be the same model you downloaded/converted in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2328694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_to_use = \"google/gemma-2-2b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfc187",
   "metadata": {},
   "source": [
    "---\n",
    "# Sanity Checking\n",
    "\n",
    "Let's do a quick sanity check to ensure we have all the pieces needed before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11920d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_MODEL=/root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo\n",
      "env: DATA_DIR=data/split\n",
      "env: TRAIN_DS=/root/ODSC-Hackathon-Repository/data/split/train.jsonl\n",
      "env: VAL_DS=/root/ODSC-Hackathon-Repository/data/split/val.jsonl\n",
      "env: RESULT_DIR=/root/ODSC-Hackathon-Repository/results\n",
      "\n",
      "################################################################################\n",
      "All checks passed. You are ready to go!\n",
      "    Base model file: /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo\n",
      "    Data directory: data/split\n",
      "    Results: /root/ODSC-Hackathon-Repository/results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_name = model_to_use.split('/')[-1].lower()\n",
    "\n",
    "# The path to the model checkpoint, and also the data directory containing the training, validation, and test data.\n",
    "nemo_model_fp = os.path.abspath(f\"models/{model_name}.nemo\")\n",
    "data_dir = \"data/split\"\n",
    "\n",
    "# The directory where the results will be stored.\n",
    "result_dir = os.path.abspath(\"results\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "# Sanity checks\n",
    "assert os.path.exists(nemo_model_fp), f\"The model checkpoint at '{nemo_model_fp}' does not exist. Please ensure the model was downloaded successfully.\"\n",
    "assert os.path.exists(data_dir), f\"The data directory '{data_dir}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "\n",
    "train_fp = os.path.abspath(f\"{data_dir}/train.jsonl\")\n",
    "val_fp = os.path.abspath(f\"{data_dir}/val.jsonl\")\n",
    "\n",
    "# Sanity checks\n",
    "assert os.path.exists(train_fp), f\"The training data at '{train_fp}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "assert os.path.exists(val_fp), f\"The validation data at '{val_fp}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "\n",
    "#\n",
    "# Set the environment variables (needed for executing the next cell)\n",
    "#\n",
    "%env BASE_MODEL=$nemo_model_fp\n",
    "%env DATA_DIR=$data_dir\n",
    "%env TRAIN_DS=$train_fp\n",
    "%env VAL_DS=$val_fp\n",
    "%env RESULT_DIR=$result_dir\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(\"All checks passed. You are ready to go!\")\n",
    "print(f\"    Base model file: {nemo_model_fp}\")\n",
    "print(f\"    Data directory: {data_dir}\")\n",
    "print(f\"    Results: {result_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d658d64",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Training\n",
    "\n",
    "With all the sanity checks passing, it is time to start model training.\n",
    "\n",
    "> NOTE: Running the following cell will remove any previously trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c05a7",
   "metadata": {},
   "source": [
    "* Our choice of bf16 precision strikes a balance between training speed and memory usage, crucial for handling our large language model. \n",
    "\n",
    "\n",
    "\n",
    "* The micro batch size of 1 and global batch size of 16 were chosen to optimize memory usage and gradient accumulation, allowing for stable training.\n",
    "\n",
    "\n",
    "\n",
    "* We implemented a cosine annealing learning rate schedule with a base rate of 1e-6. This approach helps in finding an optimal convergence point by gradually reducing the learning rate, preventing overshooting in later stages of training.\n",
    "\n",
    "\n",
    "\n",
    "* Infact, during our first trial of fine tuning the model, the training and validation loss are too far apart and also the validation is not converging while the training loss is, which is a clear sight of overfitting.\n",
    "\n",
    "\n",
    "\n",
    "* So, we introduced weight decay of 0.01 was selected to prevent overfitting and ensure smooth gradient updates.\n",
    "\n",
    "\n",
    "\n",
    "* We set our max steps to 2500 with evaluations every 200 steps. This frequent evaluation allowed us to monitor the model's performance closely and make necessary adjustments.\n",
    "\n",
    "\n",
    "\n",
    "* We have used optimizer beta values of 0.9 and 0.95 to help achieve faster convergence and improved stability during the fine-tuning of large language models by adjusting the decay rates of the gradient and variance estimates in the Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4df27e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'data/split/*idx*': No such file or directory\n",
      "[NeMo W 2024-10-28 01:10:40 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 01:10:42 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-28 01:10:42 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 2500\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 0.5\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /root/ODSC-Hackathon-Repository/results\n",
      "      exp_dir: /root/ODSC-Hackathon-Repository/results\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 16\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        chat: false\n",
      "        chat_prompt_tokens:\n",
      "          system_turn_start: \"\\0\"\n",
      "          turn_start: \"\\x11\"\n",
      "          label_start: \"\\x12\"\n",
      "          end_of_turn: '\n",
      "    \n",
      "            '\n",
      "          end_of_name: '\n",
      "    \n",
      "            '\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - /root/ODSC-Hackathon-Repository/data/split/train.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 2\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: true\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - /root/ODSC-Hackathon-Repository/data/split/val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 1\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 1.0e-06\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 200\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 01:10:42 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 01:10:42 exp_manager:450] ExpManager schema\n",
      "[NeMo I 2024-10-28 01:10:42 exp_manager:451] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False, 'save_last_n_optim_states': -1}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'log_delta_step_timing': False, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-10-28 01:10:42 exp_manager:910] exp_manager received explicit_log_dir: /root/ODSC-Hackathon-Repository/results and at least one of exp_dir: /root/ODSC-Hackathon-Repository/results, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2024-10-28 01:10:42 exp_manager:837] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/root/ODSC-Hackathon-Repository/results/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 01:10:42 exp_manager:509] Experiments will be logged at /root/ODSC-Hackathon-Repository/results\n",
      "[NeMo I 2024-10-28 01:10:42 exp_manager:1063] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 01:10:42 exp_manager:1201] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 2500. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 01:10:42 exp_manager:646] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 01:10:48 megatron_init:314] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:339] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:347] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:348] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:361] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:402] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 01:10:48 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-10-28 01:10:48 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "[NeMo I 2024-10-28 01:10:48 tokenizer_utils:197] Getting SentencePiece with model: /tmp/tmpsojsdj5n/dd4e3de1c52a49088ca428287e8b67bb_tokenizer.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 01:10:48 megatron_base_model:604] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: multi_latent_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_shared_expert_intermediate_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_shared_expert_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: cp_comm_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 01:10:48 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 01:11:01 nlp_overrides:1374] Model MegatronGPTSFTModel was successfully restored from /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo.\n",
      "[NeMo I 2024-10-28 01:11:01 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2024-10-28 01:11:01 nlp_adapter_mixins:245] Before adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 2.6 B  | train\n",
      "    ------------------------------------------------\n",
      "    0         Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,457.368Total estimated model params size (MB)\n",
      "    452       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-10-28 01:11:04 nlp_adapter_mixins:250] After adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 2.6 B  | train\n",
      "    ------------------------------------------------\n",
      "    5.3 M     Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,478.667Total estimated model params size (MB)\n",
      "    582       Modules in train mode\n",
      "    0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 01:11:04 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-28 01:11:04 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 01:11:04 megatron_gpt_sft_model:836] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:528] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:494] Building indexing for fn = /root/ODSC-Hackathon-Repository/data/split/val.jsonl\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:506] Saving idx file = /root/ODSC-Hackathon-Repository/data/split/val.jsonl.idx.npy\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:508] Saving metadata file = /root/ODSC-Hackathon-Repository/data/split/val.jsonl.idx.info\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:543] Time building 1 / 1 mem-mapped files: 0:00:00.126045\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:528] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:543] Time building 0 / 1 mem-mapped files: 0:00:00.091164\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:249] Loading /root/ODSC-Hackathon-Repository/data/split/val.jsonl\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001138\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-10-28 01:11:04 megatron_gpt_sft_model:840] Length of val dataset: 1053\n",
      "[NeMo I 2024-10-28 01:11:04 megatron_gpt_sft_model:847] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:528] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:494] Building indexing for fn = /root/ODSC-Hackathon-Repository/data/split/train.jsonl\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:506] Saving idx file = /root/ODSC-Hackathon-Repository/data/split/train.jsonl.idx.npy\n",
      "[NeMo I 2024-10-28 01:11:04 text_memmap_dataset:508] Saving metadata file = /root/ODSC-Hackathon-Repository/data/split/train.jsonl.idx.info\n",
      "[NeMo I 2024-10-28 01:11:05 text_memmap_dataset:543] Time building 1 / 1 mem-mapped files: 0:00:00.333295\n",
      "[NeMo I 2024-10-28 01:11:05 text_memmap_dataset:528] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-28 01:11:05 text_memmap_dataset:543] Time building 0 / 1 mem-mapped files: 0:00:00.096887\n",
      "[NeMo I 2024-10-28 01:11:05 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-28 01:11:05 text_memmap_dataset:249] Loading /root/ODSC-Hackathon-Repository/data/split/train.jsonl\n",
      "[NeMo I 2024-10-28 01:11:05 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000705\n",
      "[NeMo I 2024-10-28 01:11:05 text_memmap_dataset:165] Computing global indices\n",
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-10-28 01:11:05 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.05 (sec)\n",
      "[NeMo I 2024-10-28 01:11:05 megatron_gpt_sft_model:849] Length of train dataset: 40200\n",
      "[NeMo I 2024-10-28 01:11:05 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-28 01:11:05 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2024-10-28 01:11:05 megatron_base_model:1230] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 2500.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 01:11:05 nlp_adapter_mixins:329] Optimizer groups set:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 2.6 B  | train\n",
      "    ------------------------------------------------\n",
      "    5.3 M     Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,478.667Total estimated model params size (MB)\n",
      "    582       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-10-28 01:11:05 modelPT:787] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 1e-06\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-10-28 01:11:05 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fcbec3dcf10>\" \n",
      "    will be used during training (effective maximum steps = 2500) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 200\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 2500\n",
      "    )\n",
      "[NeMo I 2024-10-28 01:11:05 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fc95844f430>\" \n",
      "    will be used during training (effective maximum steps = 2500) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 200\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 2500\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | Float16Module | 2.6 B  | train\n",
      "------------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "2.6 B     Non-trainable params\n",
      "2.6 B     Total params\n",
      "10,478.667Total estimated model params size (MB)\n",
      "582       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[NeMo W 2024-10-28 01:11:05 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-10-28 01:11:17 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][NeMo I 2024-10-28 01:11:18 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:04<00:00,  0.45it/s][NeMo I 2024-10-28 01:11:22 num_microbatches_calculator:228] setting number of microbatches to constant 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 01:11:22 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 01:11:22 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 01:11:22 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 01:11:34 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo W 2024-10-28 01:11:47 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :   8%|         | 200/2500 [11:28<2:12:03, reduced_train_loss=5.160, global_step=199.0, consumed_samples=3200.0, train_step_timing in s=3.420]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 01:23:18 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:14,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<01:57,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:51,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:07<01:50,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:47,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:44,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:12<01:41,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:38,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:15<01:36,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:34,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:32,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:20<01:30,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:28,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:26,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:23,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:28<01:21,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:19,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:33<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:14,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:09,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:41<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:04,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:49<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:54<00:54,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:57<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:49,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:02<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:44,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:10<00:37,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:15<00:32,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:18<00:29,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:26,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:23<00:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:31<00:16,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:36<00:11,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:44<00:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.61it/s]\u001b[A[NeMo I 2024-10-28 01:25:05 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 6.725\n",
      "Epoch 0, global step 200: 'validation_loss' reached 6.72534 (best 6.72534), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=6.725-step=200-consumed_samples=3200.0.ckpt' as top 1\n",
      "[NeMo W 2024-10-28 01:25:05 nlp_overrides:625] DistributedCheckpointIO configured but should not be used. Reverting back to TorchCheckpointIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  16%|        | 400/2500 [24:44<2:09:55, reduced_train_loss=4.860, global_step=399.0, consumed_samples=6400.0, train_step_timing in s=3.430, val_loss=6.730]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 01:36:34 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:15,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<01:58,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:51,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:06<01:47,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:44,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:41,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:39,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:37,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:15<01:35,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:33,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:25,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:27<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:18,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:48<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:57<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:43,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:09<00:37,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:18<00:29,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:26,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:31<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:12,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 01:38:21 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.932 >= min_delta = 0.001. New best score: 4.793\n",
      "Epoch 0, global step 400: 'validation_loss' reached 4.79304 (best 4.79304), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=4.793-step=400-consumed_samples=6400.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  16%|        | 400/2500 [26:32<2:19:18, reduced_train_loss=4.860, global_step=399.0, consumed_samples=6400.0, train_step_timing in s=3.430, val_loss=4.790][NeMo I 2024-10-28 01:38:21 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=6.725-step=200-consumed_samples=3200.0.ckpt\n",
      "[NeMo I 2024-10-28 01:38:22 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=6.725-step=200-consumed_samples=3200.0-last.ckpt\n",
      "Epoch 0: :  24%|       | 600/2500 [38:00<2:00:22, reduced_train_loss=4.160, global_step=599.0, consumed_samples=9600.0, train_step_timing in s=3.420, val_loss=4.790]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 01:49:50 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:19,  0.47it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<02:00,  0.53it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:52,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:06<01:48,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:44,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:41,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:39,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:37,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:15<01:35,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:33,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:25,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:27<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:18,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:48<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:57<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:43,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:09<00:37,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:18<00:29,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:26,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:30<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:12,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 01:51:37 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.304 >= min_delta = 0.001. New best score: 3.489\n",
      "Epoch 0, global step 600: 'validation_loss' reached 3.48945 (best 3.48945), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=3.489-step=600-consumed_samples=9600.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  24%|       | 600/2500 [39:48<2:06:02, reduced_train_loss=4.160, global_step=599.0, consumed_samples=9600.0, train_step_timing in s=3.420, val_loss=3.490][NeMo I 2024-10-28 01:51:37 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=4.793-step=400-consumed_samples=6400.0.ckpt\n",
      "[NeMo I 2024-10-28 01:51:38 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=4.793-step=400-consumed_samples=6400.0-last.ckpt\n",
      "Epoch 0: :  32%|      | 800/2500 [51:16<1:48:57, reduced_train_loss=2.920, global_step=799.0, consumed_samples=12800.0, train_step_timing in s=3.420, val_loss=3.490]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 02:03:05 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:15,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<01:58,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:51,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:06<01:47,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:44,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:41,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:39,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:37,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:15<01:35,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:33,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:25,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:27<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:18,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:48<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:57<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:43,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:09<00:37,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:17<00:29,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:26,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:30<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:12,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 02:04:52 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.590 >= min_delta = 0.001. New best score: 2.899\n",
      "Epoch 0, global step 800: 'validation_loss' reached 2.89896 (best 2.89896), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.899-step=800-consumed_samples=12800.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  32%|      | 800/2500 [53:03<1:52:44, reduced_train_loss=2.920, global_step=799.0, consumed_samples=12800.0, train_step_timing in s=3.420, val_loss=2.900][NeMo I 2024-10-28 02:04:53 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=3.489-step=600-consumed_samples=9600.0.ckpt\n",
      "[NeMo I 2024-10-28 02:04:53 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=3.489-step=600-consumed_samples=9600.0-last.ckpt\n",
      "Epoch 0: :  40%|      | 1000/2500 [1:04:31<1:36:47, reduced_train_loss=2.720, global_step=999.0, consumed_samples=1.6e+4, train_step_timing in s=3.420, val_loss=2.900]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 02:16:20 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:14,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<01:57,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:51,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:06<01:47,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:44,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:41,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:39,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:36,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:14<01:34,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:32,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:25,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:27<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:18,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:48<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:57<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:43,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:09<00:37,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:17<00:29,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:25,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:30<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:12,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 02:18:07 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.185 >= min_delta = 0.001. New best score: 2.714\n",
      "Epoch 0, global step 1000: 'validation_loss' reached 2.71358 (best 2.71358), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.714-step=1000-consumed_samples=16000.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  40%|      | 1000/2500 [1:06:18<1:39:28, reduced_train_loss=2.720, global_step=999.0, consumed_samples=1.6e+4, train_step_timing in s=3.420, val_loss=2.710][NeMo I 2024-10-28 02:18:08 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.899-step=800-consumed_samples=12800.0.ckpt\n",
      "[NeMo I 2024-10-28 02:18:08 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.899-step=800-consumed_samples=12800.0-last.ckpt\n",
      "Epoch 0: :  48%|     | 1200/2500 [1:17:47<1:24:16, reduced_train_loss=2.160, global_step=1199.0, consumed_samples=19200.0, train_step_timing in s=3.430, val_loss=2.710]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 02:29:36 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:16,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<01:58,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:51,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:06<01:47,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:44,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:41,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:39,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:37,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:15<01:35,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:33,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:25,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:27<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:18,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:48<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:56<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:43,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:09<00:37,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:17<00:29,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:25,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:30<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:12,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:38<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 02:31:23 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.113 >= min_delta = 0.001. New best score: 2.600\n",
      "Epoch 0, global step 1200: 'validation_loss' reached 2.60033 (best 2.60033), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.600-step=1200-consumed_samples=19200.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  48%|     | 1200/2500 [1:19:34<1:26:11, reduced_train_loss=2.160, global_step=1199.0, consumed_samples=19200.0, train_step_timing in s=3.430, val_loss=2.600][NeMo I 2024-10-28 02:31:23 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.714-step=1000-consumed_samples=16000.0.ckpt\n",
      "[NeMo I 2024-10-28 02:31:24 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.714-step=1000-consumed_samples=16000.0-last.ckpt\n",
      "Epoch 0: :  56%|    | 1400/2500 [1:31:03<1:11:32, reduced_train_loss=2.160, global_step=1399.0, consumed_samples=22400.0, train_step_timing in s=3.420, val_loss=2.600]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 02:42:52 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:15,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<01:58,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:51,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:06<01:47,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:44,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:41,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:39,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:36,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:14<01:34,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:32,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:30,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:25,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:23,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:27<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:18,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:16,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:35<01:11,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:06,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:01,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:48<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:56,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:56<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:43,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:04<00:42,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:38,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:09<00:37,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:17<00:29,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:25,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:25<00:21,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:30<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:12,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:38<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:46<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 02:44:39 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.043 >= min_delta = 0.001. New best score: 2.557\n",
      "Epoch 0, global step 1400: 'validation_loss' reached 2.55716 (best 2.55716), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.557-step=1400-consumed_samples=22400.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  56%|    | 1400/2500 [1:32:50<1:12:56, reduced_train_loss=2.160, global_step=1399.0, consumed_samples=22400.0, train_step_timing in s=3.420, val_loss=2.560][NeMo I 2024-10-28 02:44:39 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.600-step=1200-consumed_samples=19200.0.ckpt\n",
      "[NeMo I 2024-10-28 02:44:40 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.600-step=1200-consumed_samples=19200.0-last.ckpt\n",
      "Epoch 0: :  64%|   | 1600/2500 [1:44:18<58:40, reduced_train_loss=2.530, global_step=1599.0, consumed_samples=25600.0, train_step_timing in s=3.420, val_loss=2.560]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 02:56:08 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:21,  0.46it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<02:01,  0.53it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:53,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:07<01:48,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:45,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:42,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:40,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:37,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:15<01:35,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:33,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:26,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:28<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:19,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:14,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:49<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:57<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:44,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:10<00:37,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:18<00:29,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:26,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:31<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:13,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 02:57:55 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.054 >= min_delta = 0.001. New best score: 2.503\n",
      "Epoch 0, global step 1600: 'validation_loss' reached 2.50295 (best 2.50295), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.503-step=1600-consumed_samples=25600.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  64%|   | 1600/2500 [1:46:06<59:40, reduced_train_loss=2.530, global_step=1599.0, consumed_samples=25600.0, train_step_timing in s=3.420, val_loss=2.500][NeMo I 2024-10-28 02:57:55 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.557-step=1400-consumed_samples=22400.0.ckpt\n",
      "[NeMo I 2024-10-28 02:57:56 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.557-step=1400-consumed_samples=22400.0-last.ckpt\n",
      "Epoch 0: :  72%|  | 1800/2500 [1:57:34<45:43, reduced_train_loss=1.930, global_step=1799.0, consumed_samples=28800.0, train_step_timing in s=3.420, val_loss=2.500]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 03:09:23 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:15,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<01:58,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:51,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:06<01:47,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:44,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:41,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:39,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:36,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:14<01:34,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:32,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:25,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:23,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:27<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:18,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:16,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:35<01:11,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:06,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:01,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:48<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:56<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:43,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:09<00:37,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:17<00:29,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:25,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:30<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:12,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 03:11:10 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.028 >= min_delta = 0.001. New best score: 2.475\n",
      "Epoch 0, global step 1800: 'validation_loss' reached 2.47451 (best 2.47451), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.475-step=1800-consumed_samples=28800.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  72%|  | 1800/2500 [1:59:21<46:24, reduced_train_loss=1.930, global_step=1799.0, consumed_samples=28800.0, train_step_timing in s=3.420, val_loss=2.470][NeMo I 2024-10-28 03:11:10 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.503-step=1600-consumed_samples=25600.0.ckpt\n",
      "[NeMo I 2024-10-28 03:11:11 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.503-step=1600-consumed_samples=25600.0-last.ckpt\n",
      "Epoch 0: :  80%|  | 2000/2500 [2:10:50<32:42, reduced_train_loss=1.850, global_step=2e+3, consumed_samples=3.2e+4, train_step_timing in s=3.430, val_loss=2.470]   \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 03:22:39 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:21,  0.46it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<02:01,  0.53it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:53,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:07<01:48,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:45,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:42,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:39,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:37,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:15<01:35,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:33,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:26,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:28<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:18,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:49<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:57<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:43,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:10<00:37,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:18<00:29,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:26,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:31<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:13,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 03:24:26 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 2.471\n",
      "Epoch 0, global step 2000: 'validation_loss' reached 2.47131 (best 2.47131), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.471-step=2000-consumed_samples=32000.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  80%|  | 2000/2500 [2:12:37<33:09, reduced_train_loss=1.850, global_step=2e+3, consumed_samples=3.2e+4, train_step_timing in s=3.430, val_loss=2.470][NeMo I 2024-10-28 03:24:27 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.475-step=1800-consumed_samples=28800.0.ckpt\n",
      "[NeMo I 2024-10-28 03:24:27 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.475-step=1800-consumed_samples=28800.0-last.ckpt\n",
      "Epoch 0: :  88%| | 2200/2500 [2:24:05<19:38, reduced_train_loss=2.120, global_step=2199.0, consumed_samples=35200.0, train_step_timing in s=3.430, val_loss=2.470]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 03:35:55 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:24,  0.45it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<02:02,  0.52it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:54,  0.55it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:07<01:49,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:45,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:42,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:40,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:37,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:15<01:35,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:33,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:28,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:26,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:28<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:19,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:14,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:09,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:41<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:49<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:57<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:49,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:02<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:44,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:10<00:37,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:18<00:29,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:26,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:23<00:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:31<00:16,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:44<00:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 03:37:42 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 2.470\n",
      "Epoch 0, global step 2200: 'validation_loss' reached 2.46976 (best 2.46976), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.470-step=2200-consumed_samples=35200.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  88%| | 2200/2500 [2:25:53<19:53, reduced_train_loss=2.120, global_step=2199.0, consumed_samples=35200.0, train_step_timing in s=3.430, val_loss=2.470][NeMo I 2024-10-28 03:37:42 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.471-step=2000-consumed_samples=32000.0.ckpt\n",
      "[NeMo I 2024-10-28 03:37:43 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.471-step=2000-consumed_samples=32000.0-last.ckpt\n",
      "Epoch 0: :  96%|| 2400/2500 [2:37:22<06:33, reduced_train_loss=1.980, global_step=2399.0, consumed_samples=38400.0, train_step_timing in s=3.430, val_loss=2.470]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 03:49:11 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 1/66 [00:02<02:19,  0.46it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 2/66 [00:03<02:00,  0.53it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 3/66 [00:05<01:52,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 4/66 [00:06<01:48,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 5/66 [00:08<01:44,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 6/66 [00:10<01:42,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|         | 7/66 [00:11<01:39,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 8/66 [00:13<01:37,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 9/66 [00:15<01:35,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 10/66 [00:16<01:33,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 11/66 [00:18<01:31,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 12/66 [00:19<01:29,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 13/66 [00:21<01:27,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 14/66 [00:23<01:25,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 15/66 [00:24<01:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 16/66 [00:26<01:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 17/66 [00:27<01:20,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 18/66 [00:29<01:18,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 19/66 [00:31<01:17,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 20/66 [00:32<01:15,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 21/66 [00:34<01:13,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 22/66 [00:36<01:12,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 23/66 [00:37<01:10,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 24/66 [00:39<01:08,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 25/66 [00:40<01:07,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 26/66 [00:42<01:05,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 27/66 [00:44<01:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 28/66 [00:45<01:02,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 29/66 [00:47<01:00,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 30/66 [00:48<00:58,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 31/66 [00:50<00:57,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 32/66 [00:52<00:55,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 33/66 [00:53<00:53,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 34/66 [00:55<00:52,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 35/66 [00:57<00:50,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 36/66 [00:58<00:48,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 37/66 [01:00<00:47,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 38/66 [01:01<00:45,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 39/66 [01:03<00:44,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|    | 40/66 [01:05<00:42,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 41/66 [01:06<00:40,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 42/66 [01:08<00:39,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 43/66 [01:10<00:37,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 44/66 [01:11<00:35,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 45/66 [01:13<00:34,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 46/66 [01:14<00:32,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 47/66 [01:16<00:30,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 48/66 [01:18<00:29,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 49/66 [01:19<00:27,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 50/66 [01:21<00:26,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 51/66 [01:22<00:24,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 52/66 [01:24<00:22,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 53/66 [01:26<00:21,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 54/66 [01:27<00:19,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 55/66 [01:29<00:17,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 56/66 [01:31<00:16,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 57/66 [01:32<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 58/66 [01:34<00:13,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 59/66 [01:35<00:11,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 60/66 [01:37<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 61/66 [01:39<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 62/66 [01:40<00:06,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 63/66 [01:42<00:04,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 64/66 [01:43<00:03,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 65/66 [01:45<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 66/66 [01:47<00:00,  0.62it/s]\u001b[A[NeMo I 2024-10-28 03:50:58 num_microbatches_calculator:228] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.001. New best score: 2.464\n",
      "Epoch 0, global step 2400: 'validation_loss' reached 2.46362 (best 2.46362), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.464-step=2400-consumed_samples=38400.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  96%|| 2400/2500 [2:39:09<06:37, reduced_train_loss=1.980, global_step=2399.0, consumed_samples=38400.0, train_step_timing in s=3.430, val_loss=2.460][NeMo I 2024-10-28 03:50:59 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.470-step=2200-consumed_samples=35200.0.ckpt\n",
      "[NeMo I 2024-10-28 03:50:59 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.470-step=2200-consumed_samples=35200.0-last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=2500` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|| 2500/2500 [2:44:54<00:00, reduced_train_loss=1.930, global_step=2499.0, consumed_samples=4e+4, train_step_timing in s=3.420, val_loss=2.460]   \n",
      "[NeMo I 2024-10-28 03:56:43 perf_metrics:87] TFLOPs per sec per GPU=-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-10-28 03:56:43 perf_metrics:85] Failed to calculate TFLOPs per sec per GPU.\n",
      "    FLOPs measurement not supported for finetuning jobs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 03:56:44 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.464-step=2400-consumed_samples=38400.0-last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.464-step=2400-consumed_samples=38400.0.ckpt\n",
      "Restored all states from the checkpoint at /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=2.464-step=2400-consumed_samples=38400.0.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Clear up cached mem-map file\n",
    "rm $DATA_DIR/*idx*\n",
    "# Clean up prior results\n",
    "rm -r $RESULT_DIR\n",
    "\n",
    "HYDRA_FULL_ERROR=1 torchrun --nproc_per_node=1 \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${RESULT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${RESULT_DIR} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16 \\\n",
    "    trainer.val_check_interval=200 \\\n",
    "    trainer.max_steps=2500 \\\n",
    "    trainer.gradient_clip_val=0.5 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=16 \\\n",
    "    model.optim.sched.name=\"CosineAnnealing\" \\\n",
    "    model.optim.sched.warmup_steps=200 \\\n",
    "    model.optim.lr=1e-6 \\\n",
    "    model.optim.weight_decay=0.01 \\\n",
    "    model.optim.betas=[0.9,0.95] \\\n",
    "    model.restore_from_path=${BASE_MODEL} \\\n",
    "    model.data.train_ds.num_workers=2 \\\n",
    "    model.data.train_ds.add_bos=True \\\n",
    "    model.data.validation_ds.num_workers=1 \\\n",
    "    model.data.train_ds.file_names=[${TRAIN_DS}] \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=[${VAL_DS}] \\\n",
    "    model.peft.peft_scheme=${SCHEME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a1d15",
   "metadata": {},
   "source": [
    "---\n",
    "# Inference and Submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e1b57",
   "metadata": {},
   "source": [
    "To make a submission, run inference with your model on the test dataset at `data/split/submission.jsonl`.\n",
    "\n",
    "> NOTE: This dataset was generated as part of Step 1. Please ensure it exists before proceeding.\n",
    "\n",
    "In order to do this, set the variable pointing to your submission data file in the set below, then excute the final cell.\n",
    "\n",
    "The inference results will be written under `results/inference` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c8569f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference set: /root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "Trained adapter: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
      "env: TEST_DS=/root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "env: TEST_FP=submission.jsonl\n",
      "env: TRAINED_ADAPTER=/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n"
     ]
    }
   ],
   "source": [
    "test_fp = os.path.abspath(f\"{data_dir}/submission.jsonl\")\n",
    "assert os.path.exists(test_fp), f\"The submission data at '{test_fp}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "\n",
    "test_fp = os.path.abspath(test_fp)\n",
    "adapter_fp = f\"{result_dir}/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "os.makedirs(f\"{result_dir}/inference\", exist_ok=True)\n",
    "\n",
    "print(f\"Inference set: {test_fp}\")\n",
    "print(f\"Trained adapter: {adapter_fp}\")\n",
    "test_filename = os.path.basename(test_fp)\n",
    "\n",
    "\n",
    "%env TEST_DS=$test_fp\n",
    "%env TEST_FP=$test_filename\n",
    "%env TRAINED_ADAPTER=$adapter_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fed6aa",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd60670-25d3-4a2d-af72-597656c0e083",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 05:26:12 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 05:26:13 megatron_gpt_generate:125] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-28 05:26:13 megatron_gpt_generate:126] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 1\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - /root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "          names:\n",
      "          - infer\n",
      "          global_batch_size: 32\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: ${data.train_ds.label_key}\n",
      "          add_eos: ${data.train_ds.add_eos}\n",
      "          add_sep: ${data.train_ds.add_sep}\n",
      "          add_bos: ${data.train_ds.add_bos}\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: results/inference/infer-submission.jsonl\n",
      "          truncation_field: ${data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 05:26:13 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 05:26:19 megatron_init:314] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:339] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:347] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:348] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:361] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:402] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 05:26:19 tokenizer_utils:197] Getting SentencePiece with model: /tmp/tmpmpvffa3_/dd4e3de1c52a49088ca428287e8b67bb_tokenizer.model\n",
      "[NeMo I 2024-10-28 05:26:19 megatron_base_model:604] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: multi_latent_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_shared_expert_intermediate_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_shared_expert_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: cp_comm_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 05:26:19 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 05:26:35 nlp_overrides:1374] Model MegatronGPTSFTModel was successfully restored from /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo.\n",
      "[NeMo I 2024-10-28 05:26:35 nlp_adapter_mixins:245] Before adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 2.6 B  | train\n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,457.368Total estimated model params size (MB)\n",
      "    451       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-10-28 05:26:38 nlp_adapter_mixins:250] After adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 2.6 B  | train\n",
      "    -------------------------------------------\n",
      "    5.3 M     Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,478.667Total estimated model params size (MB)\n",
      "    581       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-10-28 05:26:38 megatron_gpt_generate:156] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type     | Params | Mode\n",
      "    ------------------------------------------\n",
      "    0 | model | GPTModel | 2.6 B  | eval\n",
      "    ------------------------------------------\n",
      "    0         Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,478.667Total estimated model params size (MB)\n",
      "    0         Modules in train mode\n",
      "    581       Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 05:26:38 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-28 05:26:38 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 05:26:38 megatron_gpt_sft_model:828] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-10-28 05:26:38 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-28 05:26:38 text_memmap_dataset:528] Processing 1 data files using 6 workers\n",
      "[NeMo I 2024-10-28 05:26:38 text_memmap_dataset:494] Building indexing for fn = /root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "[NeMo I 2024-10-28 05:26:38 text_memmap_dataset:506] Saving idx file = /root/ODSC-Hackathon-Repository/data/split/submission.jsonl.idx.npy\n",
      "[NeMo I 2024-10-28 05:26:38 text_memmap_dataset:508] Saving metadata file = /root/ODSC-Hackathon-Repository/data/split/submission.jsonl.idx.info\n",
      "[NeMo I 2024-10-28 05:26:38 text_memmap_dataset:543] Time building 1 / 1 mem-mapped files: 0:00:00.214818\n",
      "[NeMo I 2024-10-28 05:26:38 text_memmap_dataset:528] Processing 1 data files using 6 workers\n",
      "[NeMo I 2024-10-28 05:26:39 text_memmap_dataset:543] Time building 0 / 1 mem-mapped files: 0:00:00.201747\n",
      "[NeMo I 2024-10-28 05:26:39 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-28 05:26:39 text_memmap_dataset:249] Loading /root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "[NeMo I 2024-10-28 05:26:39 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000865\n",
      "[NeMo I 2024-10-28 05:26:39 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-10-28 05:26:39 megatron_gpt_sft_model:831] Length of test dataset: 5000\n",
      "[NeMo I 2024-10-28 05:26:39 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2024-10-28 05:26:39 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   0%|          | 0/157 [00:00<?, ?it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   1%|          | 1/157 [00:19<50:16,  0.05it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   1%|         | 2/157 [00:35<45:37,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   2%|         | 3/157 [00:49<42:42,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   3%|         | 4/157 [01:04<40:48,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   3%|         | 5/157 [01:19<40:31,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   4%|         | 6/157 [01:36<40:18,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   4%|         | 7/157 [01:51<39:46,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   5%|         | 8/157 [02:08<39:45,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   6%|         | 9/157 [02:22<39:04,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   6%|         | 10/157 [02:36<38:27,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   7%|         | 11/157 [02:55<38:47,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   8%|         | 12/157 [03:11<38:32,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   8%|         | 13/157 [03:24<37:48,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:   9%|         | 14/157 [03:43<38:01,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  10%|         | 15/157 [03:59<37:47,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  10%|         | 16/157 [04:17<37:46,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  11%|         | 17/157 [04:32<37:24,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  11%|        | 18/157 [04:50<37:23,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  12%|        | 19/157 [05:04<36:51,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  13%|        | 20/157 [05:23<36:53,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  13%|        | 21/157 [05:42<36:58,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  14%|        | 22/157 [06:00<36:49,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  15%|        | 23/157 [06:16<36:32,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  15%|        | 24/157 [06:34<36:26,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  16%|        | 25/157 [06:53<36:22,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  17%|        | 26/157 [07:12<36:17,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  17%|        | 27/157 [07:29<36:04,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  18%|        | 28/157 [07:44<35:41,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  18%|        | 29/157 [07:57<35:07,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  19%|        | 30/157 [08:12<34:46,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  20%|        | 31/157 [08:32<34:41,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  20%|        | 32/157 [08:46<34:16,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  21%|        | 33/157 [09:00<33:50,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  22%|       | 34/157 [09:15<33:28,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  22%|       | 35/157 [09:32<33:16,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  23%|       | 36/157 [09:48<32:58,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  24%|       | 37/157 [10:05<32:42,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  24%|       | 38/157 [10:21<32:26,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  25%|       | 39/157 [10:38<32:11,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  25%|       | 40/157 [10:55<31:57,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  26%|       | 41/157 [11:12<31:42,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  27%|       | 42/157 [11:26<31:20,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  27%|       | 43/157 [11:43<31:04,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  28%|       | 44/157 [12:02<30:54,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  29%|       | 45/157 [12:20<30:44,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  29%|       | 46/157 [12:39<30:32,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  30%|       | 47/157 [12:59<30:25,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  31%|       | 48/157 [13:13<30:01,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  31%|       | 49/157 [13:34<29:54,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  32%|      | 50/157 [13:49<29:34,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  32%|      | 51/157 [14:10<29:26,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  33%|      | 52/157 [14:26<29:09,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  34%|      | 53/157 [14:40<28:46,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  34%|      | 54/157 [14:56<28:29,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  35%|      | 55/157 [15:14<28:16,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  36%|      | 56/157 [15:28<27:55,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  36%|      | 57/157 [15:46<27:40,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  37%|      | 58/157 [16:02<27:23,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  38%|      | 59/157 [16:20<27:09,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  38%|      | 60/157 [16:38<26:53,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  39%|      | 61/157 [16:56<26:39,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  39%|      | 62/157 [17:12<26:22,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  40%|      | 63/157 [17:28<26:04,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  41%|      | 64/157 [17:51<25:56,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  41%|     | 65/157 [18:05<25:36,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  42%|     | 66/157 [18:22<25:20,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  43%|     | 67/157 [18:44<25:10,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  43%|     | 68/157 [18:58<24:50,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  44%|     | 69/157 [19:17<24:36,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  45%|     | 70/157 [19:33<24:17,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  45%|     | 71/157 [19:50<24:01,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  46%|     | 72/157 [20:08<23:46,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  46%|     | 73/157 [20:32<23:38,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  47%|     | 74/157 [20:48<23:20,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  48%|     | 75/157 [21:03<23:01,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  48%|     | 76/157 [21:21<22:45,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  49%|     | 77/157 [21:36<22:27,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  50%|     | 78/157 [21:50<22:07,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  50%|     | 79/157 [22:11<21:54,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  51%|     | 80/157 [22:25<21:35,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  52%|    | 81/157 [22:42<21:18,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  52%|    | 82/157 [23:01<21:03,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  53%|    | 83/157 [23:19<20:47,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  54%|    | 84/157 [23:35<20:29,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  54%|    | 85/157 [23:52<20:13,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  55%|    | 86/157 [24:09<19:56,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  55%|    | 87/157 [24:28<19:41,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  56%|    | 88/157 [24:42<19:22,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  57%|    | 89/157 [24:57<19:04,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  57%|    | 90/157 [25:16<18:49,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  58%|    | 91/157 [25:34<18:32,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  59%|    | 92/157 [25:51<18:16,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  59%|    | 93/157 [26:06<17:58,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  60%|    | 94/157 [26:24<17:41,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  61%|    | 95/157 [26:44<17:27,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  61%|    | 96/157 [27:00<17:09,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  62%|   | 97/157 [27:16<16:52,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  62%|   | 98/157 [27:34<16:35,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  63%|   | 99/157 [27:47<16:16,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  64%|   | 100/157 [28:07<16:01,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  64%|   | 101/157 [28:23<15:44,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  65%|   | 102/157 [28:38<15:26,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  66%|   | 103/157 [28:55<15:09,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  66%|   | 104/157 [29:14<14:54,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  67%|   | 105/157 [29:30<14:36,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  68%|   | 106/157 [29:48<14:20,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  68%|   | 107/157 [30:05<14:03,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  69%|   | 108/157 [30:23<13:47,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  69%|   | 109/157 [30:40<13:30,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  70%|   | 110/157 [30:57<13:13,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  71%|   | 111/157 [31:13<12:56,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  71%|  | 112/157 [31:30<12:39,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  72%|  | 113/157 [31:47<12:22,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  73%|  | 114/157 [32:05<12:06,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  73%|  | 115/157 [32:19<11:48,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  74%|  | 116/157 [32:40<11:32,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  75%|  | 117/157 [32:56<11:15,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  75%|  | 118/157 [33:16<10:59,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  76%|  | 119/157 [33:30<10:41,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  76%|  | 120/157 [33:48<10:25,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  77%|  | 121/157 [34:04<10:08,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  78%|  | 122/157 [34:19<09:50,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  78%|  | 123/157 [34:34<09:33,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  79%|  | 124/157 [34:49<09:15,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  80%|  | 125/157 [35:04<08:58,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  80%|  | 126/157 [35:19<08:41,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  81%|  | 127/157 [35:43<08:26,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  82%| | 128/157 [36:01<08:09,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  82%| | 129/157 [36:18<07:52,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  83%| | 130/157 [36:41<07:37,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  83%| | 131/157 [36:59<07:20,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  84%| | 132/157 [37:13<07:02,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  85%| | 133/157 [37:26<06:45,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  85%| | 134/157 [37:43<06:28,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  86%| | 135/157 [38:01<06:11,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  87%| | 136/157 [38:17<05:54,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  87%| | 137/157 [38:35<05:37,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  88%| | 138/157 [38:52<05:21,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  89%| | 139/157 [39:08<05:04,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  89%| | 140/157 [39:25<04:47,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  90%| | 141/157 [39:42<04:30,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  90%| | 142/157 [39:58<04:13,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  91%| | 143/157 [40:13<03:56,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  92%|| 144/157 [40:32<03:39,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  92%|| 145/157 [40:52<03:23,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  93%|| 146/157 [41:11<03:06,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  94%|| 147/157 [41:27<02:49,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  94%|| 148/157 [41:43<02:32,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  95%|| 149/157 [41:57<02:15,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  96%|| 150/157 [42:15<01:58,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  96%|| 151/157 [42:32<01:41,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  97%|| 152/157 [42:52<01:24,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  97%|| 153/157 [43:05<01:07,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  98%|| 154/157 [43:24<00:50,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  99%|| 155/157 [43:40<00:33,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0:  99%|| 156/157 [43:55<00:16,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0: 100%|| 157/157 [44:14<00:00,  0.06it/s][NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:553] skipping autogenerated example example \n",
      "    You are an expert in Law and also in tagging legal questions.\n",
      "    You are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\n",
      "    \n",
      "    You are also provided with a list of all the tags, enclosed in ^^^^^.\n",
      "    \n",
      "    Your task is to:\n",
      "    i. Understand the question and it's title.\n",
      "    ii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\n",
      "    iii. Make sure you return the tags alone without their description.\n",
      "    \n",
      "    \n",
      "    ```\n",
      "    NOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    ```\n",
      "    \n",
      "    Your output should be a JSON with the below format:\n",
      "    ```\n",
      "    tags : <Put your relevant tags here>\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    >>>>>\n",
      "    Title: Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    >>>>>\n",
      "    \n",
      "    \n",
      "    +++++\n",
      "    Question: Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government?\n",
      "    +++++\n",
      "    \n",
      "     prediction ^\n",
      "    tags: law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, parliament, law, constitution, label  \n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:586] Total deduplicated inference data size: 5024 to 5000\n",
      "[NeMo I 2024-10-28 06:10:53 megatron_gpt_sft_model:737] Predictions saved to results/inference/infer-submission.jsonl_test_infer_inputs_preds_labels.jsonl\n",
      "setting number of microbatches to constant 32\n",
      "Testing DataLoader 0: 100%|| 157/157 [44:14<00:00,  0.06it/s]\n",
      "\n",
      "\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
      "\n",
      "\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    5.409348487854004    \u001b[0m\u001b[35m \u001b[0m\n",
      "\u001b[36m \u001b[0m\u001b[36m     test_loss_infer     \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    5.409348487854004    \u001b[0m\u001b[35m \u001b[0m\n",
      "\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    5.409348487854004    \u001b[0m\u001b[35m \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 06:10:53 megatron_gpt_sft_model:677] No training data found, reconfiguring microbatches based on validation batch sizes.\n",
      "[NeMo W 2024-10-28 06:10:53 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 06:10:53 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss_infer', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 06:10:53 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# This is where the inference results will be stored.\n",
    "OUTPUT_DIR=\"results/inference/infer-$TEST_FP\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Clear up cached mem-map file\n",
    "rm $DATA_DIR/*idx*\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${BASE_MODEL} \\\n",
    "    model.peft.restore_from_path=${TRAINED_ADAPTER} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.file_names=[${TEST_DS}] \\\n",
    "    model.data.test_ds.names=[\"infer\"] \\\n",
    "    model.data.test_ds.global_batch_size=32 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=32 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.data.test_ds.output_file_path_prefix=$OUTPUT_DIR \\\n",
    "    model.data.test_ds.write_predictions_to_file=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5e7ee",
   "metadata": {},
   "source": [
    "The results will be written under `results/inference`. Please send us this file for your final submission.\n",
    "\n",
    "Let's inspect a couple of lines from that file for sanity checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b4726de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"\\nYou are an expert in Law and also in tagging legal questions.\\nYou are provided with a question enclosed in +++++ and it's corresponding title enclosed in >>>>> from the law domain.\\n\\nYou are also provided with a list of all the tags, enclosed in ^^^^^.\\n\\nYour task is to:\\ni. Understand the question and it's title.\\nii. Pick up the tags that are most appropriate and relevant to the question, strictly from the tags provided to you.\\niii. Make sure you return the tags alone without their description.\\n\\n\\n```\\nNOTES: All tags must be in lowercase, ordered lexicographically and separated by commas.\\n```\\n\\nYour output should be a JSON with the below format:\\n```\\ntags : <Put your relevant tags here>\\n```\\n\\n\\n>>>>>\\nTitle: Fairness in Punishment for Reckless Behavior\\n>>>>>\\n\\n\\n+++++\\nQuestion: Is it justifiable to have significantly different penalties for individuals who engage in reckless behavior, depending on the outcome of their actions, or should the focus be on the level of recklessness itself, regardless of the consequences?\\n+++++\\n\\n\", \"pred\": \"^^^^^\\ntags:\\nlaw\\npunishment\\nreckless\\nbehavior\\nfairness\\njustification\\noutcome\\nlevel\\nrecklessness\\n\", \"label\": \" \", \"filename\": \"submission.jsonl\"}\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! cat results/inference/infer-submission.jsonl_test_infer_inputs_preds_labels.jsonl | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928b7cd-a976-4a87-aefe-1617ce3c6413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58554da2",
   "metadata": {},
   "source": [
    "---\n",
    "# Freeing Memory and Other Resources\n",
    "\n",
    "As always, it is a good idea to free up all allocated resources when you are done. Please execute the following cell to do so.\n",
    "\n",
    "Alternatively, please restart the kernel by navigating to `Kernel > Restart Kernel` (if using Jypyter notebook), or clicking the `Restart` button in VS Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf196ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
